apiVersion: v1
kind: Secret
metadata:
  name: huggingface-secret
  namespace: test
type: Opaque
data:
  HF_TOKEN: ""
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: run-config
  namespace: test
data:
  config.yaml: |
    version: '2'
    image_name: remote-vllm
    apis:
    - datasetio
    - eval
    - inference
    - scoring
    - telemetry
    - tool_runtime
    - vector_io
    providers:
      inference:
      - provider_id: vllm-inference
        provider_type: remote::vllm
        config:
          url: ${env.VLLM_URL}
          max_tokens: ${env.VLLM_MAX_TOKENS}
          api_token: fake
      - provider_id: sentence-transformers
        provider_type: inline::sentence-transformers
        config: {}
      vector_io:
      - provider_id: faiss
        provider_type: inline::faiss
        config:
          kvstore:
            type: sqlite
            namespace: null
            db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/remote-vllm}/faiss_store.db
      eval:
      - provider_id: lmeval
        provider_type: remote::lmeval
        config: {}
      # datasetio:
      # - provider_id: huggingface
      #   provider_type: remote::huggingface
      #   config: {}
      # - provider_id: localfs
      #   provider_type: inline::localfs
      #   config: {}
      # scoring:
      # - provider_id: basic
      #   provider_type: inline::basic
      #   config: {}
      # - provider_id: llm-as-judge
      #   provider_type: inline::llm-as-judge
      #   config: {}
      # - provider_id: braintrust
      #   provider_type: inline::braintrust
      #   config:
      #     openai_api_key: ${env.OPENAI_API_KEY:}
      telemetry:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          service_name: ${env.OTEL_SERVICE_NAME:llama-stack}
          sinks: ${env.TELEMETRY_SINKS:console,sqlite}
          sqlite_db_path: ${env.SQLITE_DB_PATH:~/.llama/distributions/remote-vllm/trace_store.db}
      tool_runtime:
      # - provider_id: brave-search
      #   provider_type: remote::brave-search
      #   config:
      #     api_key: ${env.BRAVE_SEARCH_API_KEY:}
      #     max_results: 3
      - provider_id: tavily-search
        provider_type: remote::tavily-search
        config:
          api_key: ${env.TAVILY_SEARCH_API_KEY:}
          max_results: 3
      # - provider_id: code-interpreter
      #   provider_type: inline::code-interpreter
      #   config: {}
      - provider_id: rag-runtime
        provider_type: inline::rag-runtime
        config: {}
      - provider_id: model-context-protocol
        provider_type: remote::model-context-protocol
        config: {}
    metadata_store:
      type: sqlite
      db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/remote-vllm}/registry.db
    models:
    - metadata: {}
      model_id: ${env.INFERENCE_MODEL}
      provider_id: vllm-inference
      model_type: llm
    - metadata:
        embedding_dimension: 384
      model_id: all-MiniLM-L6-v2
      provider_id: sentence-transformers
      model_type: embedding
    vector_dbs: []
    datasets: []
    scoring_fns: []
    eval_tasks: []
    tool_groups:
    - toolgroup_id: builtin::websearch
      provider_id: tavily-search
    - toolgroup_id: builtin::rag
      provider_id: rag-runtime
    # - toolgroup_id: builtin::code_interpreter
    #   provider_id: code-interpreter
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: template
  namespace: test
  labels:
    app: vllm
data:
  tool_chat_template_llama3.2_json.jinja: |
    {{- bos_token }} {%- if custom_tools is defined %}
        {%- set tools = custom_tools %}
    {%- endif %} {%- if not tools_in_user_message is defined %}
        {%- set tools_in_user_message = false %}
    {%- endif %} {%- if not date_string is defined %}
        {%- if strftime_now is defined %}
            {%- set date_string = strftime_now("%d %b %Y") %}
        {%- else %}
            {%- set date_string = "26 Jul 2024" %}
        {%- endif %}
    {%- endif %} {%- if not tools is defined %}
        {%- set tools = none %}
    {%- endif %}
    {#- Find out if there are any images #} {% set image_ns = namespace(has_images=false) %} {%- for message in messages %}
        {%- for content in message['content'] %}
            {%- if content['type'] == 'image' %}
                {%- set image_ns.has_images = true %}
            {%- endif %}
        {%- endfor %}
    {%- endfor %}
    {#- This block extracts the system message, so we can slot it into the right place. #} {%- if messages[0]['role'] == 'system' %}
        {%- if messages[0]['content'] is string %}
            {%- set system_message = messages[0]['content']|trim %}
        {%- else %}
            {%- set system_message = messages[0]['content'][0]['text']|trim %}
        {%- endif %}
        {%- set messages = messages[1:] %}
    {%- else %}
        {%- if tools is not none %}
            {%- set system_message = "You are a helpful assistant with tool calling capabilities. Only reply with a tool call if the function exists in the library provided by the user. If it doesn't exist, just reply directly in natural language. When you receive a tool call response, use the output to format an answer to the original user question." %}
        {%- else %}
            {%- set system_message = "" %}
        {%- endif %}
    {%- endif %}
    {#- System message if there are no images, if the user supplied one, or if tools are used (default tool system message) #} {%- if system_message or not image_ns.has_images %}
        {{- "<|start_header_id|>system<|end_header_id|>\n\n" }}
        {%- if tools is not none %}
            {{- "Environment: ipython\n" }}
        {%- endif %}
        {{- "Cutting Knowledge Date: December 2023\n" }}
        {{- "Today Date: " + date_string + "\n\n" }}
        {%- if tools is not none and not tools_in_user_message %}
            {{- "You have access to the following functions. To call a function, please respond with JSON for a function call. " }}
            {{- 'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}. ' }}
            {{- "Do not use variables.\n\n" }}
            {%- for t in tools %}
                {{- t | tojson(indent=4) }}
                {{- "\n\n" }}
            {%- endfor %}
        {%- endif %}
        {{- system_message }}
        {{- "<|eot_id|>" }}
    {%- endif %}
    {#- Custom tools are passed in a user message with some extra guidance #} {%- if tools_in_user_message and not tools is none %}
        {#- Extract the first user message so we can plug it in here #}
        {%- if messages | length != 0 %}
            {%- if messages[0]['content'] is string %}
                {%- set first_user_message = messages[0]['content']|trim %}
            {%- else %}
                {%- set first_user_message = messages[0]['content'] | selectattr('type', 'equalto', 'text') | map(attribute='text') | map('trim') | join('\n') %}
            {%- endif %}
            {%- set messages = messages[1:] %}
        {%- else %}
            {{- raise_exception("Cannot put tools in the first user message when there's no first user message!") }}
        {%- endif %}
        {{- '<|start_header_id|>user<|end_header_id|>\n\n' -}}
        {{- "Given the following functions, please respond with a JSON for a function call " }}
        {{- "with its proper arguments that best answers the given prompt.\n\n" }}
        {{- 'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}. ' }}
        {{- "Do not use variables.\n\n" }}
        {%- for t in tools %}
            {{- t | tojson(indent=4) }}
            {{- "\n\n" }}
        {%- endfor %}
        {{- first_user_message + "<|eot_id|>"}}
    {%- endif %}
    {%- for message in messages %}
        {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}
            {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n' }}
            {%- if message['content'] is string %}
                {{- message['content'] | trim}}
            {%- else %}
                {%- for content in message['content'] %}
                    {%- if content['type'] == 'image' %}
                        {{- '<|image|>' }}
                    {%- elif content['type'] == 'text' %}
                        {{- content['text'] | trim }}
                    {%- endif %}
                {%- endfor %}
            {%- endif %}
            {{- '<|eot_id|>' }}
        {%- elif 'tool_calls' in message %}
            {%- if not message.tool_calls|length == 1 %}
                {{- raise_exception("This model only supports single tool-calls at once!") }}
            {%- endif %}
            {%- set tool_call = message.tool_calls[0].function %}
            {{- '<|start_header_id|>assistant<|end_header_id|>\n\n' -}}
            {{- '{"name": "' + tool_call.name + '", ' }}
            {{- '"parameters": ' }}
            {{- tool_call.arguments | tojson }}
            {{- "}" }}
            {{- "<|eot_id|>" }}
        {%- elif message.role == "tool" or message.role == "ipython" %}
            {{- "<|start_header_id|>ipython<|end_header_id|>\n\n" }}
            {%- if message.content is string %}
                {{- { "output": message.content } | tojson }}
            {%- else %}
                {%- for content in message['content']  %}
                    {%- if content['type']  == 'text' %}
                        {{- { "output": content['text']  } | tojson }}
                    {%- endif %}
                {%- endfor %}
            {%- endif %}
            {{- "<|eot_id|>" }}
        {%- endif %}
    {%- endfor %} {%- if add_generation_prompt %}
        {{- '<|start_header_id|>assistant<|end_header_id|>\n\n' }}
    {%- endif %}
---
apiVersion: v1
kind: Service
metadata:
  name: llamastack-service
  namespace: test
spec:
  type: ClusterIP
  ports:
    - port: 8321
      targetPort: 8321
      protocol: TCP
      name: http
  selector:
    app: llamastack
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llamastack-deployment
  namespace: test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llamastack
  template:
    metadata:
      labels:
        app: llamastack
    spec:
      containers:
      - args:
        - --yaml-config
        - /app-config/config.yaml
        env:
        - name: VLLM_MAX_TOKENS
          value: "4096"
        - name: INFERENCE_MODEL
          value: tinyllama
        - name: VLLM_URL
          value: http://vllm-server:8000/v1
        - name: VLLM_API_TOKEN
          value: fake
        - name:  SAFETY_MODEL
          value: tinyllama
        image: quay.io/ruimvieira/distribution-lmeval:dev
        imagePullPolicy: IfNotPresent
        name: llamastack
        ports:
        - containerPort: 8321
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /app-config
          name: run-config-volume
        - mountPath: /.llama
          name: llama-temp
        - mountPath: /.cache
          name: cache
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
      volumes:
      - configMap:
          defaultMode: 420
          name: run-config
        name: run-config-volume
      - emptyDir: {}
        name: llama-temp
      - emptyDir: {}
        name: cache