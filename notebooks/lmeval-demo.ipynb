{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building Custom Distribution of LLamaStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import pprint\n",
    "\n",
    "from llama_stack.distribution.library_client import LlamaStackAsLibraryClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_http_client():\n",
    "    from llama_stack_client import LlamaStackClient\n",
    "    return LlamaStackClient(base_url=\"http://localhost:8321\")\n",
    "\n",
    "client = create_http_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provider: lmeval\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sanity check for lmeval as a registered provider\n",
    "providers = client.providers.list()\n",
    "for provider in providers:\n",
    "    if provider.api == \"eval\":\n",
    "        print(f\"Provider: {provider.provider_id}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run LM-Eval\n",
    "\n",
    "Start by listing the currently registered benchmarks (this should be empty, for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "URL = \"http://0.0.0.0:8321/v1/eval\"\n",
    "\n",
    "benchmarks = client.benchmarks.list()\n",
    "\n",
    "print(benchmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register a new benchmark. We'll register the MMLU benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.benchmarks.register(\n",
    "    benchmark_id=\"lmeval::arc_easy\",\n",
    "    dataset_id=\"lmeval::arc_easy\",\n",
    "    scoring_functions=[\"string\"],\n",
    "    provider_benchmark_id=\"string\",\n",
    "    provider_id=\"lmeval\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the benchmark was properly registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Benchmark(dataset_id='lmeval::arc_easy', identifier='lmeval::arc_easy', metadata={}, provider_id='lmeval', provider_resource_id='string', scoring_functions=['string'], type='benchmark')]\n"
     ]
    }
   ],
   "source": [
    "benchmarks = client.benchmarks.list()\n",
    "\n",
    "print(benchmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job 'lmeval-job-0'\n"
     ]
    }
   ],
   "source": [
    "job = client.eval.run_eval(\n",
    "    benchmark_id=\"lmeval::arc_easy\",\n",
    "    benchmark_config={\n",
    "        \"eval_candidate\": {\n",
    "            \"type\": \"model\",\n",
    "            \"model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "            \"provider_id\": \"lmeval\",\n",
    "            \"sampling_params\": {\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 0.9,\n",
    "                \"max_tokens\": 256\n",
    "            },\n",
    "        },\n",
    "        \"num_examples\": 1000 # Just for testing\n",
    "    },)\n",
    "\n",
    "print(f\"Starting job '{job.job_id}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's periodically poll the benchmark results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job(job_id='lmeval-job-0', status='scheduled')\n",
      "Job(job_id='lmeval-job-0', status='in_progress')\n",
      "Job(job_id='lmeval-job-0', status='in_progress')\n",
      "Job(job_id='lmeval-job-0', status='in_progress')\n",
      "Job(job_id='lmeval-job-0', status='in_progress')\n",
      "Job(job_id='lmeval-job-0', status='in_progress')\n",
      "Job(job_id='lmeval-job-0', status='in_progress')\n",
      "Job(job_id='lmeval-job-0', status='in_progress')\n",
      "Job(job_id='lmeval-job-0', status='in_progress')\n",
      "Job(job_id='lmeval-job-0', status='in_progress')\n",
      "Job(job_id='lmeval-job-0', status='in_progress')\n",
      "Job(job_id='lmeval-job-0', status='in_progress')\n",
      "Job(job_id='lmeval-job-0', status='in_progress')\n",
      "Job(job_id='lmeval-job-0', status='completed')\n",
      "Job ended with status: completed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def get_job_status():\n",
    "    return client.eval.jobs.status(job_id=job.job_id, benchmark_id=\"lmeval::arc_easy\")\n",
    "\n",
    "while True:\n",
    "    job = get_job_status()\n",
    "    print(job)\n",
    "\n",
    "    if job.status in ['failed', 'completed']:\n",
    "        print(f\"Job ended with status: {job.status}\")\n",
    "        break\n",
    "\n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get Job Results\n",
    "\n",
    "Let's get this job's scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arc_easy:acc': ScoringResult(aggregated_results={'acc': 0.259}, score_rows=[{'score': 0.259}]),\n",
      " 'arc_easy:acc_norm': ScoringResult(aggregated_results={'acc_norm': 0.255}, score_rows=[{'score': 0.255}]),\n",
      " 'arc_easy:acc_norm_stderr': ScoringResult(aggregated_results={'acc_norm_stderr': 0.013790038620872835}, score_rows=[{'score': 0.013790038620872835}]),\n",
      " 'arc_easy:acc_stderr': ScoringResult(aggregated_results={'acc_stderr': 0.01386041525752791}, score_rows=[{'score': 0.01386041525752791}])}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(client.eval.jobs.retrieve(job_id=job.job_id, benchmark_id=\"lmeval::arc_easy\").scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, we can now delete the evaluation job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "client.eval.jobs.cancel(job_id=job.job_id, benchmark_id=\"lmeval::arc_easy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
