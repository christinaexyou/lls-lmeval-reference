{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building Custom Distribution of LLamaStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T12:08:05.175784Z",
     "start_time": "2025-04-17T12:08:04.888383Z"
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "from llama_stack.distribution.library_client import LlamaStackAsLibraryClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T12:08:06.443067Z",
     "start_time": "2025-04-17T12:08:06.419913Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_http_client():\n",
    "    from llama_stack_client import LlamaStackClient\n",
    "    return LlamaStackClient(base_url=\"http://localhost:8321\")\n",
    "\n",
    "client = create_http_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T12:08:07.761407Z",
     "start_time": "2025-04-17T12:08:07.753336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provider: lmeval\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sanity check for lmeval as a registered provider\n",
    "providers = client.providers.list()\n",
    "for provider in providers:\n",
    "    if provider.api == \"eval\":\n",
    "        print(f\"Provider: {provider.provider_id}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run LM-Eval\n",
    "\n",
    "Start by listing the currently registered benchmarks (this should be empty, for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T12:08:10.205983Z",
     "start_time": "2025-04-17T12:08:10.200007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "benchmarks = client.benchmarks.list()\n",
    "\n",
    "print(benchmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register a new benchmark. We'll register the MMLU benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T12:08:12.580916Z",
     "start_time": "2025-04-17T12:08:12.352497Z"
    }
   },
   "outputs": [],
   "source": [
    "client.benchmarks.register(\n",
    "    benchmark_id=\"lmeval::arc_easy\",\n",
    "    dataset_id=\"lmeval::arc_easy\",\n",
    "    scoring_functions=[\"string\"],\n",
    "    provider_benchmark_id=\"string\",\n",
    "    provider_id=\"lmeval\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the benchmark was properly registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T12:08:14.047738Z",
     "start_time": "2025-04-17T12:08:14.043704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Benchmark(dataset_id='lmeval::arc_easy', identifier='lmeval::arc_easy', metadata={}, provider_id='lmeval', provider_resource_id='string', scoring_functions=['string'], type='benchmark')]\n"
     ]
    }
   ],
   "source": [
    "benchmarks = client.benchmarks.list()\n",
    "\n",
    "print(benchmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store the model name in `MODEL`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T12:08:16.545805Z",
     "start_time": "2025-04-17T12:08:16.079299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job 'lmeval-job-1a77bd75-2ef3-4ef6-9778-6c7af4a9b08b'\n"
     ]
    }
   ],
   "source": [
    "job = client.eval.run_eval(\n",
    "    benchmark_id=\"lmeval::arc_easy\",\n",
    "    benchmark_config={\n",
    "        \"eval_candidate\": {\n",
    "            \"type\": \"model\",\n",
    "            \"model\": MODEL,\n",
    "            \"provider_id\": \"lmeval\",\n",
    "            \"sampling_params\": {\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 0.9,\n",
    "                \"max_tokens\": 256\n",
    "            },\n",
    "        },\n",
    "        \"num_examples\": 1000 # Just for testing\n",
    "    },)\n",
    "\n",
    "print(f\"Starting job '{job.job_id}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's periodically poll the benchmark results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-17T12:08:19.119596Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job(job_id='lmeval-job-1a77bd75-2ef3-4ef6-9778-6c7af4a9b08b', status='scheduled')\n",
      "Job(job_id='lmeval-job-1a77bd75-2ef3-4ef6-9778-6c7af4a9b08b', status='in_progress')\n",
      "Job(job_id='lmeval-job-1a77bd75-2ef3-4ef6-9778-6c7af4a9b08b', status='in_progress')\n",
      "Job(job_id='lmeval-job-1a77bd75-2ef3-4ef6-9778-6c7af4a9b08b', status='in_progress')\n",
      "Job(job_id='lmeval-job-1a77bd75-2ef3-4ef6-9778-6c7af4a9b08b', status='in_progress')\n",
      "Job(job_id='lmeval-job-1a77bd75-2ef3-4ef6-9778-6c7af4a9b08b', status='in_progress')\n",
      "Job(job_id='lmeval-job-1a77bd75-2ef3-4ef6-9778-6c7af4a9b08b', status='in_progress')\n",
      "Job(job_id='lmeval-job-1a77bd75-2ef3-4ef6-9778-6c7af4a9b08b', status='in_progress')\n",
      "Job(job_id='lmeval-job-1a77bd75-2ef3-4ef6-9778-6c7af4a9b08b', status='in_progress')\n",
      "Job(job_id='lmeval-job-1a77bd75-2ef3-4ef6-9778-6c7af4a9b08b', status='in_progress')\n",
      "Job(job_id='lmeval-job-1a77bd75-2ef3-4ef6-9778-6c7af4a9b08b', status='in_progress')\n",
      "Job(job_id='lmeval-job-1a77bd75-2ef3-4ef6-9778-6c7af4a9b08b', status='completed')\n",
      "Job ended with status: completed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def get_job_status(job_id, benchmark_id):\n",
    "    return client.eval.jobs.status(job_id=job_id, benchmark_id=benchmark_id)\n",
    "\n",
    "while True:\n",
    "    job = get_job_status(job_id=job.job_id, benchmark_id=\"lmeval::arc_easy\")\n",
    "    print(job)\n",
    "\n",
    "    if job.status in ['failed', 'completed']:\n",
    "        print(f\"Job ended with status: {job.status}\")\n",
    "        break\n",
    "\n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get Job Results\n",
    "\n",
    "Let's get this job's scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arc_easy:acc': ScoringResult(aggregated_results={'acc': 0.259}, score_rows=[{'score': 0.259}]),\n",
      " 'arc_easy:acc_norm': ScoringResult(aggregated_results={'acc_norm': 0.256}, score_rows=[{'score': 0.256}]),\n",
      " 'arc_easy:acc_norm_stderr': ScoringResult(aggregated_results={'acc_norm_stderr': 0.013807775152234194}, score_rows=[{'score': 0.013807775152234194}]),\n",
      " 'arc_easy:acc_stderr': ScoringResult(aggregated_results={'acc_stderr': 0.01386041525752791}, score_rows=[{'score': 0.01386041525752791}])}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(client.eval.jobs.retrieve(job_id=job.job_id, benchmark_id=\"lmeval::arc_easy\").scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, we can now delete the evaluation job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "client.eval.jobs.cancel(job_id=job.job_id, benchmark_id=\"lmeval::arc_easy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dk-bench\n",
    "\n",
    "Create the `dk-bench` benchmark.\n",
    "This will use the same model for the judge, for simplicity purposes.\n",
    "Any other model could be passed using `JUDGE_MODEL_URL` and `JUDGE_MODEL_NAME`, either by passing the service directly or by using a Llama Stack OpenAI inference endpoint.\n",
    "\n",
    "The `dk-bench` task is defined at the `https://github.com/trustyai-explainability/lm-eval-tasks.git` repository, under the `tasks` directory.\n",
    "\n",
    "We also assume a PVC (in this case named `my-pvc`) exists in the same namespace and it contains the necessary datasets under the directory `upload-files`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.benchmarks.register(\n",
    "    benchmark_id=\"lmeval::dk-bench\",\n",
    "    dataset_id=\"lmeval::dk-bench\",\n",
    "    scoring_functions=[\"string\"],\n",
    "    provider_benchmark_id=\"string\",\n",
    "    provider_id=\"lmeval\",\n",
    "    metadata={\n",
    "        \"custom_task\": {\n",
    "            \"git\": {\n",
    "                \"url\": \"https://github.com/trustyai-explainability/lm-eval-tasks.git\",\n",
    "                \"branch\": \"main\",\n",
    "                \"commit\": \"2ff52c6560b14fbf3ec141b1357b076e80b8f25a\",\n",
    "                \"path\": \"tasks/\",\n",
    "            }\n",
    "        },\n",
    "        \"env\": {\n",
    "            \"DK_BENCH_DATASET_PATH\": \"/opt/app-root/src/hf_home/upload-files/example-dk-bench-input-bmo.jsonl\",\n",
    "            \"JUDGE_MODEL_URL\": \"http://vllm-server:8000/v1/chat/completions\",\n",
    "            \"JUDGE_MODEL_NAME\": MODEL,\n",
    "            \"JUDGE_API_KEY\": \"\",\n",
    "        },\n",
    "        \"input\": {\"storage\": {\"pvc\": \"my-pvc\"}}\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the available benchmarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Benchmark(dataset_id='lmeval::arc_easy', identifier='lmeval::arc_easy', metadata={}, provider_id='lmeval', provider_resource_id='string', scoring_functions=['string'], type='benchmark'), Benchmark(dataset_id='lmeval::dk-bench', identifier='lmeval::dk-bench', metadata={'custom_task': {'git': {'url': 'https://github.com/trustyai-explainability/lm-eval-tasks.git', 'branch': 'main', 'commit': '2ff52c6560b14fbf3ec141b1357b076e80b8f25a', 'path': 'tasks/'}}, 'env': {'DK_BENCH_DATASET_PATH': '/opt/app-root/src/hf_home/upload-files/example-dk-bench-input-bmo.jsonl', 'JUDGE_MODEL_URL': 'http://vllm-server:8000/v1/chat/completions', 'JUDGE_MODEL_NAME': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'JUDGE_API_KEY': ''}, 'input': {'storage': {'pvc': 'my-pvc'}}}, provider_id='lmeval', provider_resource_id='string', scoring_functions=['string'], type='benchmark')]\n"
     ]
    }
   ],
   "source": [
    "benchmarks = client.benchmarks.list()\n",
    "\n",
    "print(benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job 'lmeval-job-519f7493-874a-4da3-b38c-e19bd58c0025'\n"
     ]
    }
   ],
   "source": [
    "job = client.eval.run_eval(\n",
    "    benchmark_id=\"lmeval::dk-bench\",\n",
    "    benchmark_config={\n",
    "        \"eval_candidate\": {\n",
    "            \"type\": \"model\",\n",
    "            \"model\": MODEL,\n",
    "            \"provider_id\": \"lmeval\",\n",
    "            \"sampling_params\": {\"temperature\": 0.7, \"top_p\": 0.9, \"max_tokens\": 256},\n",
    "        },\n",
    "        \"num_examples\": 1000,  # Just for testing\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Starting job '{job.job_id}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job(job_id='lmeval-job-519f7493-874a-4da3-b38c-e19bd58c0025', status='scheduled')\n",
      "Job(job_id='lmeval-job-519f7493-874a-4da3-b38c-e19bd58c0025', status='in_progress')\n",
      "Job(job_id='lmeval-job-519f7493-874a-4da3-b38c-e19bd58c0025', status='in_progress')\n",
      "Job(job_id='lmeval-job-519f7493-874a-4da3-b38c-e19bd58c0025', status='in_progress')\n",
      "Job(job_id='lmeval-job-519f7493-874a-4da3-b38c-e19bd58c0025', status='failed')\n",
      "Job ended with status: failed\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    job = get_job_status(job_id=job.job_id, benchmark_id=\"lmeval::dk-bench\")\n",
    "    print(job)\n",
    "\n",
    "    if job.status in ['failed', 'completed']:\n",
    "        print(f\"Job ended with status: {job.status}\")\n",
    "        break\n",
    "\n",
    "    time.sleep(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
